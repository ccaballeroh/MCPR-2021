{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "oriented-cinema",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "\n",
    "This notebook extracts several sets of features from the processed corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dense-ideal",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-accountability",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/', force_remount=True)\n",
    "    ROOT = Path(r\"./drive/My Drive/translator-attribution\")\n",
    "    sys.path.insert(0,f\"{ROOT}/\")\n",
    "else:\n",
    "    from helper import ROOT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "after-assembly",
   "metadata": {},
   "source": [
    "## Retrieving Processed Documents from Disk\n",
    "\n",
    "We can pick up the process from this step retrieving the processed documents from disk. Since the pickle file stores `Path` objects as properties of the documents, there's a difference between Windows and Linux (POSIX) paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-retro",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "import platform\n",
    "\n",
    "PICKLE = Path(fr\"{ROOT}/auxfiles/pickle/\")\n",
    "docs = {}\n",
    "\n",
    "for author in [\"Quixote\", \"Ibsen\"]:\n",
    "    with open(PICKLE/f\"{author}_{platform.system()}.pickle\", \"rb\") as f:\n",
    "        doc_data = f.read()\n",
    "    docs[author] = pickle.loads(doc_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-albuquerque",
   "metadata": {},
   "source": [
    "### For the *Quixote* corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-insertion",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from helper.analysis import save_dataset_to_json\n",
    "\n",
    "author = \"Quixote\"\n",
    "\n",
    "# syntactic n-grams with n in {2,3}\n",
    "for n in range(2, 4):\n",
    "    FILE_TEMPLATE = f\"{author}_syntactic_n{n}\"\n",
    "    save_dataset_to_json([\n",
    "        (doc.n_grams_syntactic(n=n), doc.translator) for doc in docs[author]\n",
    "    ], FILE_TEMPLATE)\n",
    "    \n",
    "for punct in [True, False]:\n",
    "    # word n-grams with and without punctuation with n in {1, 2, 3}\n",
    "    for n in range(1, 4):\n",
    "        FILE_TEMPLATE = f\"{author}_{n}grams{'_punct' if punct else ''}\"\n",
    "        save_dataset_to_json([\n",
    "            (doc.n_grams(n=n, punct=punct, pos=False), doc.translator) for doc in docs[author]\n",
    "        ], FILE_TEMPLATE)\n",
    "    # POS n-grams with and without punctuation with n in {2, 3}\n",
    "    for n in range(2, 4):\n",
    "        FILE_TEMPLATE = f\"{author}_{n}gramsPOS{'_punct' if punct else ''}\"\n",
    "        save_dataset_to_json([\n",
    "            (doc.n_grams(n=n, punct=punct, pos=True), doc.translator) for doc in docs[author]\n",
    "        ], FILE_TEMPLATE)\n",
    "    # Cohesive markers with and without punctuation\n",
    "    for _ in range(1):\n",
    "        FILE_TEMPLATE = f\"{author}_cohesive{'_punct' if punct else ''}\"\n",
    "        save_dataset_to_json([\n",
    "            (doc.cohesive(punct=punct), doc.translator) for doc in docs[author]\n",
    "        ], FILE_TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-jerusalem",
   "metadata": {},
   "source": [
    "### For the Ibsen Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunrise-oakland",
   "metadata": {},
   "source": [
    "#### Extract Features from Parallel Corpus (i.e., *Ghosts*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discrete-causing",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from helper.analysis import save_dataset_to_json\n",
    "\n",
    "author = \"Ibsen\"\n",
    "\n",
    "# syntactic n_grams with n in {2, 3}\n",
    "for n in range(2, 4):\n",
    "    FILE_TEMPLATE = f\"{author}_Ghosts_syntactic_n{n}\"\n",
    "    save_dataset_to_json([\n",
    "        (doc.n_grams_syntactic(n=n), doc.translator)\n",
    "        for doc in docs[author]\n",
    "        if \"Ghosts\" in doc.filename\n",
    "    ], FILE_TEMPLATE)\n",
    "    \n",
    "for punct in [True, False]:\n",
    "    \n",
    "    # word n-grams with and without punctuation with n in {1, 2, 3}\n",
    "    for n in range(1, 4):\n",
    "        FILE_TEMPLATE = f\"{author}_Ghosts_{n}grams{'_punct' if punct else ''}\"\n",
    "        save_dataset_to_json([\n",
    "            (doc.n_grams(n=n, punct=punct, pos=False, propn=False), doc.translator)\n",
    "            for doc in docs[author]\n",
    "            if \"Ghosts\" in doc.filename\n",
    "        ], FILE_TEMPLATE)\n",
    "        \n",
    "    #POS n-grams with and without punctuation with n in {2, 3}\n",
    "    for n in range(2, 4):\n",
    "        FILE_TEMPLATE = f\"{author}_Ghosts_{n}gramsPOS{'_punct' if punct else ''}\"\n",
    "        save_dataset_to_json([\n",
    "            (doc.n_grams(n=n, punct=punct, pos=True), doc.translator)\n",
    "            for doc in docs[author]\n",
    "            if \"Ghosts\" in doc.filename\n",
    "        ], FILE_TEMPLATE)\n",
    "        \n",
    "    # Cohesive markers with and without punctuation\n",
    "    for _ in range(1):\n",
    "        FILE_TEMPLATE = f\"{author}_Ghosts_cohesive{'_punct' if punct else ''}\"\n",
    "        save_dataset_to_json([\n",
    "            (doc.cohesive(punct=punct), doc.translator)\n",
    "            for doc in docs[author]\n",
    "            if \"Ghosts\" in doc.filename\n",
    "        ], FILE_TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-fitting",
   "metadata": {},
   "source": [
    "#### Extract Features from Non-Parallel Corpus (i.e., the other plays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-forth",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from helper.analysis import save_dataset_to_json\n",
    "\n",
    "author = \"Ibsen\"\n",
    "\n",
    "# syntactic n-grams with n in {2, 3}\n",
    "for n in range(2, 4):\n",
    "    FILE_TEMPLATE = f\"{author}_Other_syntactic_n{n}\"\n",
    "    save_dataset_to_json([\n",
    "        (doc.n_grams_syntactic(n=n, propn=False), doc.translator)\n",
    "        for doc in docs[author]\n",
    "        if not \"Ghosts\" in doc.filename\n",
    "    ], FILE_TEMPLATE)\n",
    "\n",
    "for punct in [True, False]:\n",
    "    \n",
    "    # word n-grams with and without punctuation with n in {1, 2, 3}\n",
    "    for n in range(1, 4):\n",
    "        FILE_TEMPLATE = f\"{author}_Others_{n}grams{'_punct' if punct else ''}\"\n",
    "        save_dataset_to_json([\n",
    "            (doc.n_grams(n=n, punct=punct, pos=False, propn=False), doc.translator)\n",
    "            for doc in docs[author]\n",
    "            if not \"Ghosts\" in doc.filename\n",
    "        ], FILE_TEMPLATE)\n",
    "    \n",
    "    # POS n-grams with and without punctuation with n in {2, 3}\n",
    "    for n in range(2, 4):\n",
    "        FILE_TEMPLATE = f\"{author}_Others_{n}gramsPOS{'_punct' if punct else ''}\"\n",
    "        save_dataset_to_json([\n",
    "            (doc.n_grams(n=n, punct=punct, pos=True), doc.translator)\n",
    "            for doc in docs[author]\n",
    "            if not \"Ghosts\" in doc.filename\n",
    "        ], FILE_TEMPLATE)\n",
    "    \n",
    "    # Cohesive markers with and without punctuation\n",
    "    for _ in range(1):\n",
    "        FILE_TEMPLATE = f\"{author}_Others_cohesive{'_punct' if punct else ''}\"\n",
    "        save_dataset_to_json([\n",
    "            (doc.cohesive(punct=punct), doc.translator)\n",
    "            for doc in docs[author]\n",
    "            if not \"Ghosts\" in doc.filename\n",
    "        ], FILE_TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-updating",
   "metadata": {},
   "source": [
    "## Cleaning (Optional)\n",
    "\n",
    "We can delete from disk the files generated during the preprocessing and syntactic feature extraction steps in the folders `Corpora/Proc_{author}` and `auxfiles/txt/{author}` using the custom function `clean_files` in the `utils` submodule in the `helper` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developing-cricket",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper.utils import clean_files\n",
    "\n",
    "clean_files()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.7 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "cc57166f195f9b7cc74392531f5d5c2cea4e492296671de2d2c11a59bc61468d"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}