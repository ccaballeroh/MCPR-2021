{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-tumor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concrete-burning",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/', force_remount=True)\n",
    "    ROOT = Path(r\"./drive/My Drive/translator-attribution\")\n",
    "    sys.path.insert(0, f\"{ROOT}/\")\n",
    "else:\n",
    "    from helper import ROOT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classical-simulation",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "The *Quixote* files were retrieved from professor Hussein Abbass's [website](http://www.husseinabbass.net/translator.html). The files are plain text files&mdash;one file per chapter of the two parts of the novel&mdash;and only require minor preprocessing: removal of bracketed numbers, collapsing of spaces to only one whitespace, and the replacement of special characters, such as é and ü.\n",
    "\n",
    "The Ibsen files were retrieved from [Project Gutenberg](http://www.gutenberg.org). Therefore, the files contain legal information that needs to be removed along with bracketed numbers, collapsing of spaces, and the replacement of special characters also. However, before doing that, the plays were splitted in 5 kB chunks.\n",
    "\n",
    "These operations are encapsulated in two functions, `quixote()` and `ibsen()`, respectively, within the submodule `preprocessing` available in the `helper` module. The functions use the relative paths to the folders containing the raw files (`Raw_Quixote` and `Raw_Ibsen`) in the subfolder `Corpora` and ouput the processed files in the folders `Proc_Quixote` and `Proc_Ibsen`. Not necessary if already have been preprocessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-schedule",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "warming-pastor",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing.quixote()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upset-miracle",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing.ibsen()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "damaged-aspect",
   "metadata": {},
   "source": [
    "# Processing\n",
    "\n",
    "The processing of the files comprises generating an object of the custom class `MyDoc` available in the `analysis` submodule  in the `helper` module for each document in both corpora. In order to instantiate the objects, a spaCy language model has to be given. A Python list with each object is serialized and saved to disk using Python's `pickle` protocol. \n",
    "\n",
    "**Note:** If the Notebook is being run on Colab, spaCy must be installed first and the English language model downloaded. After that, it is necessary to restart the runtime and run the first cells where the Drive is mounted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "academic-subscription",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    !pip install spacy=2.2.2\n",
    "    !python -m spacy download en_core_web_md\n",
    "else:\n",
    "    try:\n",
    "        import spacy\n",
    "        nlp = spacy.load(\"en_core_web_md\")\n",
    "    except:\n",
    "        !python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "leading-indianapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import ROOT\n",
    "from helper.analysis import MyDoc\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import spacy\n",
    "import platform\n",
    "\n",
    "CORPORA = Path(fr\"{ROOT}/Corpora/\")\n",
    "PICKLE = Path(fr\"{ROOT}/auxfiles/pickle/\")\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "if not PICKLE.exists():\n",
    "    PICKLE.mkdir()\n",
    "\n",
    "docs = {}\n",
    "\n",
    "for author in [\"Quixote\", \"Ibsen\"]:\n",
    "    path = CORPORA/f\"Proc_{author}\"\n",
    "    docs[author] = [\n",
    "        MyDoc(file, nlp) for file in path.iterdir() if file.suffix==\".txt\" and file.stat().st_size != 0\n",
    "    ]\n",
    "    # save to disk\n",
    "    doc_data = pickle.dumps(docs[author])\n",
    "    with open(PICKLE/f\"{author}_{platform.system()}.pickle\", \"wb\") as f:\n",
    "        f.write(doc_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cohesive-markers]",
   "language": "python",
   "name": "conda-env-cohesive-markers-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
