{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04Extraction_Most_Relevant_Features.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python [conda env:mcpr2021]",
      "language": "python",
      "name": "conda-env-mcpr2021-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccaballeroh/MCPR-2021/blob/main/04Extraction_Most_Relevant_Features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jZ3uI9mRKh2"
      },
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "IN_COLAB = \"google.colab\" in sys.modules"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HK4MsUARPCG"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive/')\n",
        "    ROOT = Path(r\"./drive/My Drive/Translator-Attribution\")\n",
        "    sys.path.insert(0,f\"{ROOT}/\")\n",
        "else:\n",
        "    from helper.analysis import ROOT"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxRkXRnu6ymW"
      },
      "source": [
        "# Extraction of Most Relevant Features\n",
        "\n",
        "On this notebook, we extract the most relevant features in the classification process for each translator. In order to do this, we can retrieve the learned weights from a linear classifier (e.g., Logistic Regression, although a Support Vector Machine using a linear *kernel* also have those properties as well as the Na√Øve Bayes classifier) and get the $n$ largest. The corresponding $n$ features would thus be the most relevant for each class. In case of a binary classifier, the $n$ largest weights would correspond to the *positive* class, whereas the $n$ most negative weights would correspond to the *negative* class.\n",
        "\n",
        "Since scikit-learn trains $N$ binary classifiers when given an N-class multiclass problem, we can retrieve the $n$ largest weights&mdash;and their corresponding features&mdash;for each classifier. This notebook saves to disk the $n$ most relevant features for each translator in the corpora for each feature set for a logistic regression classifier. The results are saved as bar plots and also tabular ($\\LaTeX$) in the `results\\figs\\most` and `results\\tables` folders respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COM-qODNRCqx"
      },
      "source": [
        "from helper.features import convert_data, plot_most_relevant, train_extract_most_relevant, save_tables"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkkRVjg86ymq"
      },
      "source": [
        "These are the files to process. They are the entirety of the feature sets obtained using [01Processing](./01Processing.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4F_KDYeF6ymr"
      },
      "source": [
        "from helper.analysis import JSON_FOLDER"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2spuxmx76ymv"
      },
      "source": [
        "## Most Relevant Features\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpRIncSsLSfJ"
      },
      "source": [
        "We use $\\chi^2$ statistic to select the $k$ most distinctive features in each feature set to train a Logistic Regression classifier. Then we extract the $n$ most important feature for each class (i.e., translator)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOJ1kKwV6ym1",
        "outputId": "17cfc26a-6480-4424-ca63-364ec4fab218"
      },
      "source": [
        "most_relevant = {}\n",
        "num_of_features = 25\n",
        "n_most_relevant = 10\n",
        "args = {\"k\":num_of_features, \"n\":n_most_relevant}\n",
        "\n",
        "for author in [\"Ibsen\", \"Quixote\"]:\n",
        "    features_files = [file for file in JSON_FOLDER.iterdir() if file.name.startswith(author)]\n",
        "    for file in features_files:\n",
        "        data = convert_data(file=file)\n",
        "        data = {**data, **args}\n",
        "        most_relevant[file.stem] = train_extract_most_relevant(**data) \n",
        "\n",
        "        for translator in data[\"encoder\"].classes_:\n",
        "            plot_most_relevant(data=most_relevant[file.stem], translator=translator, file=file)\n",
        "            df = most_relevant[file.stem][translator]\n",
        "            save_tables(df=df, translator=translator, file=file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWTGeRlrLSfK"
      },
      "source": [
        "## Results\n",
        "\n",
        "As an example, we can compare bigrams among the parallel translations of Ibsen (i.e., *Ghosts*)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-iAEVvtLSfK"
      },
      "source": [
        "key = \"_\".join(\n",
        "    [\"Ibsen\",\n",
        "     \"Ghosts\",\n",
        "     \"2grams\",\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eatDiYQTLSfL",
        "outputId": "a3448d11-af3d-4477-f084-c1bf23ef1d00"
      },
      "source": [
        "for translator in most_relevant[key].keys():\n",
        "    print(translator, \":\\n\", most_relevant[key][translator], 2*\"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sharp :\n",
            "         Feature    Weight\n",
            "1       was the  0.615094\n",
            "2         as if  0.568035\n",
            "3         up to  0.450067\n",
            "4        do n't  0.439230\n",
            "5   manders and  0.429741\n",
            "6   manders but  0.399228\n",
            "7         it is  0.339203\n",
            "8        at all  0.292298\n",
            "9          i am  0.077720\n",
            "10     going to  0.037981 \n",
            "\n",
            "\n",
            "Archer :\n",
            "        Feature    Weight\n",
            "1         i 'm  1.091468\n",
            "2        i 've  0.913881\n",
            "3       do not  0.747682\n",
            "4    PROPN why  0.698373\n",
            "5      can not  0.638415\n",
            "6   PROPN then  0.623570\n",
            "7    not PROPN  0.501947\n",
            "8    very well  0.425225\n",
            "9     there 's  0.407276\n",
            "10   to morrow  0.362066 \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wk58oatMLSfL"
      },
      "source": [
        "Another example, from the other corpus. We show the 10 most distinctive cohesive markers sorrounded by their corresponding punctuation marks for each translator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYCWbQ4GLSfM",
        "outputId": "85e57210-f554-4d44-b6e4-e70cb8ae6304"
      },
      "source": [
        "key = \"_\".join(\n",
        "    [\"Quixote\",\n",
        "     \"cohesive\",\n",
        "     \"punct\"       \n",
        "    ])\n",
        "for translator in most_relevant[key].keys():\n",
        "    print(translator, \":\\n\", most_relevant[key][translator], 2*\"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Jarvis :\n",
            "         Feature    Weight\n",
            "1   . in short,  2.111955\n",
            "2        ; and,  1.072222\n",
            "3   immediately  0.429081\n",
            "4       , since  0.389144\n",
            "5          also  0.353554\n",
            "6         ; and  0.335309\n",
            "7         : and  0.330906\n",
            "8         . and  0.326769\n",
            "9         \" and  0.209485\n",
            "10      , while  0.192520 \n",
            "\n",
            "\n",
            "Ormsby :\n",
            "        Feature    Weight\n",
            "1   , however,  0.913814\n",
            "2      , while  0.624157\n",
            "3         \"and  0.397329\n",
            "4         \"but  0.335526\n",
            "5        \" and  0.280612\n",
            "6          and  0.237583\n",
            "7       'there  0.033889\n",
            "8     although  0.001344\n",
            "9        , and -0.084581\n",
            "10  , although -0.167373 \n",
            "\n",
            "\n",
            "Shelton :\n",
            "        Feature    Weight\n",
            "1         'and  1.651980\n",
            "2        , yet  1.586467\n",
            "3         'but  0.972124\n",
            "4   , although  0.819939\n",
            "5     likewise  0.737546\n",
            "6          yet  0.640972\n",
            "7       'there  0.581394\n",
            "8      , since  0.428931\n",
            "9    therefore  0.334524\n",
            "10      , and,  0.325530 \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}